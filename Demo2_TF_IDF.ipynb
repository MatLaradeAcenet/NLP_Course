{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 2, Term Frequency - Inverse Document Frequency\n",
    "\n",
    "TF-IDF is similar to bag of words, however the final counts are divided by the number of documents in the corpus.\n",
    "\n",
    "It can be useful for identifying words that are not well represented by your corpus of data, that may need more samples provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import os\n",
    "try:\n",
    "    os.chdir(\"Sonnets\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same data-loader as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shakespeares-sonnets_TXT_FolgerShakespeare.txt\n",
      "stopwords.txt\n"
     ]
    }
   ],
   "source": [
    "file_list = os.listdir('.')\n",
    "for file in file_list:\n",
    "    if file.startswith(\"Sonnet\") and file.endswith(\".txt\"):\n",
    "        pass\n",
    "    else:\n",
    "        print(file)\n",
    "        file_list.remove(file)\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must tell the TF-IDF vectorizer algorithm what kind of data to expect, in this case, we're using files, so we give the algorithm \"filename\" as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_algorithm = TfidfVectorizer(\"filename\")\n",
    "tfidf = tf_idf_algorithm.fit_transform(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_tfidf = tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154, 3074)\n"
     ]
    }
   ],
   "source": [
    "print(numpy_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "(3074,)\n"
     ]
    }
   ],
   "source": [
    "first_sonnet = numpy_tfidf[0]\n",
    "print(first_sonnet)\n",
    "print(first_sonnet.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   7,  101,  137,  139,  197,  199,  210,  350,  362,  371,  376,\n",
       "         378,  442,  516,  520,  556,  570,  617,  669,  692,  771,  800,\n",
       "         812,  909,  919,  930,  964, 1003, 1020, 1081, 1085, 1091, 1108,\n",
       "        1143, 1167, 1262, 1270, 1290, 1360, 1365, 1512, 1515, 1588, 1593,\n",
       "        1631, 1644, 1731, 1738, 1757, 1791, 1798, 1801, 1825, 1902, 2168,\n",
       "        2183, 2262, 2315, 2438, 2442, 2515, 2562, 2604, 2616, 2617, 2618,\n",
       "        2628, 2636, 2644, 2648, 2669, 2670, 2677, 2685, 2697, 2897, 2908,\n",
       "        2937, 2993, 2996, 3023], dtype=int64),)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find non-zero\n",
    "np.nonzero(numpy_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "astonished\n"
     ]
    }
   ],
   "source": [
    "#Un-tokenize word\n",
    "print(tf_idf_algorithm.get_feature_names()[151])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   7,  101,  137,  139,  197,  199,  210,  350,  362,  371,  376,\n",
      "        378,  442,  516,  520,  556,  570,  617,  669,  692,  771,  800,\n",
      "        812,  909,  919,  930,  964, 1003, 1020, 1081, 1085, 1091, 1108,\n",
      "       1143, 1167, 1262, 1270, 1290, 1360, 1365, 1512, 1515, 1588, 1593,\n",
      "       1631, 1644, 1731, 1738, 1757, 1791, 1798, 1801, 1825, 1902, 2168,\n",
      "       2183, 2262, 2315, 2438, 2442, 2515, 2562, 2604, 2616, 2617, 2618,\n",
      "       2628, 2636, 2644, 2648, 2669, 2670, 2677, 2685, 2697, 2897, 2908,\n",
      "       2937, 2993, 2996, 3023], dtype=int64),)\n",
      "(154,)\n"
     ]
    }
   ],
   "source": [
    "#print out the tf-idf value for a word\n",
    "print(np.nonzero(numpy_tfidf[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154,)\n",
      "[0.11337761 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.11211088 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.11779593 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09275451 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "0.4360389257241719\n"
     ]
    }
   ],
   "source": [
    "print(numpy_tfidf[:,7].shape)\n",
    "print(numpy_tfidf[:,7])\n",
    "print(np.sum(numpy_tfidf[:,7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF = []\n",
    "for i in range(0,153):\n",
    "    TF_IDF.append(np.sum(numpy_tfidf[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10969418531007252\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "print(min(TF_IDF))\n",
    "print(TF_IDF.index(min(TF_IDF)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afar\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf_algorithm.get_feature_names()[52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.10969419 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(numpy_tfidf[:,52])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess, deaccent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copied from Demo #1, we need to first put the data into a Bag of Words, then convert it over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 3), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 2), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 2), (53, 1), (54, 1), (55, 1), (56, 2), (57, 1), (58, 1), (59, 2), (60, 1), (61, 1), (62, 2), (63, 2), (64, 6), (65, 1), (66, 1), (67, 2), (68, 1), (69, 2), (70, 4), (71, 1), (72, 1), (73, 4), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 3)]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sonnets = []\n",
    "for sonnet in file_list:\n",
    "    tokenized_sonnets.append((simple_preprocess(deaccent(open(sonnet, 'r').read()))))\n",
    "    \n",
    "dictionary = corpora.Dictionary()\n",
    "BoW = []\n",
    "for sonnet in tokenized_sonnets: \n",
    "    BoW.append(dictionary.doc2bow(sonnet, allow_update=True))\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in tokenized_sonnets]\n",
    "print(bow_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=154, num_nnz=12166)\n"
     ]
    }
   ],
   "source": [
    "gensim_tfidf = TfidfModel(bow_corpus)\n",
    "print(gensim_tfidf)\n",
    "vector = gensim_tfidf[bow_corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.12136892489684474), (1, 0.004639780481169176), (2, 0.04740961375271943), (3, 0.024817735990948636), (4, 0.022615224507605128), (5, 0.08218363822383518), (6, 0.043996797901172585), (7, 0.09832471682821257), (8, 0.1444131329654769), (9, 0.16745734103410906), (10, 0.022979786414189754), (11, 0.05838418524463214), (12, 0.1444131329654769), (13, 0.1444131329654769), (14, 0.1444131329654769), (15, 0.1444131329654769), (16, 0.09832471682821257), (17, 0.1309331353865112), (18, 0.09440892973891331), (19, 0.08484471924924684), (20, 0.10788892731787901), (21, 0.1444131329654769), (22, 0.113950346895122), (23, 0.049257075948727934), (24, 0.1309331353865112), (25, 0.16745734103410906), (26, 0.1444131329654769), (27, 0.1309331353865112), (28, 0.16745734103410906), (29, 0.102764070087715), (30, 0.03133772511025979), (31, 0.16745734103410906), (32, 0.16745734103410906), (33, 0.16745734103410906), (34, 0.1309331353865112), (35, 0.1309331353865112), (36, 0.16745734103410906), (37, 0.067262891763637), (38, 0.0034069817259370938), (39, 0.12136892489684474), (40, 0.08773747901502621), (41, 0.102764070087715), (42, 0.1444131329654769), (43, 0.08773747901502621), (44, 0.102764070087715), (45, 0.12938654189278812), (46, 0.07971986201908283), (47, 0.16745734103410906), (48, 0.05022078877890167), (49, 0.10788892731787901), (50, 0.038070799141320937), (51, 0.113950346895122), (52, 0.12643087621476512), (53, 0.10788892731787901), (54, 0.1444131329654769), (55, 0.10788892731787901), (56, 0.1505610175191608), (57, 0.05550901618892855), (58, 0.113950346895122), (59, 0.1024265467348567), (60, 0.16745734103410906), (61, 0.03945623980034498), (62, 0.21577785463575802), (63, 0.011264872032431884), (64, 0.027749468986125247), (65, 0.022615224507605128), (66, 0.1444131329654769), (67, 0.10658362088940827), (68, 0.025741288842771857), (69, 0.041905179665080845), (70, 0.08381035933016169), (71, 0.08218363822383518), (72, 0.04319565637148495), (73, 0.008008088199765933), (74, 0.07326499684753383), (75, 0.10788892731787901), (76, 0.08773747901502621), (77, 0.052236300690948236), (78, 0.014689067719830466), (79, 0.09090613882648983), (80, 0.1940798128391822)]\n"
     ]
    }
   ],
   "source": [
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sonnet_001.txt', 'Sonnet_002.txt', 'Sonnet_003.txt', 'Sonnet_004.txt', 'Sonnet_005.txt', 'Sonnet_006.txt', 'Sonnet_007.txt', 'Sonnet_008.txt', 'Sonnet_009.txt', 'Sonnet_010.txt', 'Sonnet_011.txt', 'Sonnet_012.txt', 'Sonnet_013.txt', 'Sonnet_014.txt', 'Sonnet_015.txt', 'Sonnet_016.txt', 'Sonnet_017.txt', 'Sonnet_018.txt', 'Sonnet_019.txt', 'Sonnet_020.txt', 'Sonnet_021.txt', 'Sonnet_022.txt', 'Sonnet_023.txt', 'Sonnet_024.txt', 'Sonnet_025.txt', 'Sonnet_026.txt', 'Sonnet_027.txt', 'Sonnet_028.txt', 'Sonnet_029.txt', 'Sonnet_030.txt', 'Sonnet_031.txt', 'Sonnet_032.txt', 'Sonnet_033.txt', 'Sonnet_034.txt', 'Sonnet_035.txt', 'Sonnet_036.txt', 'Sonnet_037.txt', 'Sonnet_038.txt', 'Sonnet_039.txt', 'Sonnet_040.txt', 'Sonnet_041.txt', 'Sonnet_042.txt', 'Sonnet_043.txt', 'Sonnet_044.txt', 'Sonnet_045.txt', 'Sonnet_046.txt', 'Sonnet_047.txt', 'Sonnet_048.txt', 'Sonnet_049.txt', 'Sonnet_050.txt', 'Sonnet_051.txt', 'Sonnet_052.txt', 'Sonnet_053.txt', 'Sonnet_054.txt', 'Sonnet_055.txt', 'Sonnet_056.txt', 'Sonnet_057.txt', 'Sonnet_058.txt', 'Sonnet_059.txt', 'Sonnet_060.txt', 'Sonnet_061.txt', 'Sonnet_062.txt', 'Sonnet_063.txt', 'Sonnet_064.txt', 'Sonnet_065.txt', 'Sonnet_066.txt', 'Sonnet_067.txt', 'Sonnet_068.txt', 'Sonnet_069.txt', 'Sonnet_070.txt', 'Sonnet_071.txt', 'Sonnet_072.txt', 'Sonnet_073.txt', 'Sonnet_074.txt', 'Sonnet_075.txt', 'Sonnet_076.txt', 'Sonnet_077.txt', 'Sonnet_078.txt', 'Sonnet_079.txt', 'Sonnet_080.txt', 'Sonnet_081.txt', 'Sonnet_082.txt', 'Sonnet_083.txt', 'Sonnet_084.txt', 'Sonnet_085.txt', 'Sonnet_086.txt', 'Sonnet_087.txt', 'Sonnet_088.txt', 'Sonnet_089.txt', 'Sonnet_090.txt', 'Sonnet_091.txt', 'Sonnet_092.txt', 'Sonnet_093.txt', 'Sonnet_094.txt', 'Sonnet_095.txt', 'Sonnet_096.txt', 'Sonnet_097.txt', 'Sonnet_098.txt', 'Sonnet_099.txt', 'Sonnet_100.txt', 'Sonnet_101.txt', 'Sonnet_102.txt', 'Sonnet_103.txt', 'Sonnet_104.txt', 'Sonnet_105.txt', 'Sonnet_106.txt', 'Sonnet_107.txt', 'Sonnet_108.txt', 'Sonnet_109.txt', 'Sonnet_110.txt', 'Sonnet_111.txt', 'Sonnet_112.txt', 'Sonnet_113.txt', 'Sonnet_114.txt', 'Sonnet_115.txt', 'Sonnet_116.txt', 'Sonnet_117.txt', 'Sonnet_118.txt', 'Sonnet_119.txt', 'Sonnet_120.txt', 'Sonnet_121.txt', 'Sonnet_122.txt', 'Sonnet_123.txt', 'Sonnet_124.txt', 'Sonnet_125.txt', 'Sonnet_126.txt', 'Sonnet_127.txt', 'Sonnet_128.txt', 'Sonnet_129.txt', 'Sonnet_130.txt', 'Sonnet_131.txt', 'Sonnet_132.txt', 'Sonnet_133.txt', 'Sonnet_134.txt', 'Sonnet_135.txt', 'Sonnet_136.txt', 'Sonnet_137.txt', 'Sonnet_138.txt', 'Sonnet_139.txt', 'Sonnet_140.txt', 'Sonnet_141.txt', 'Sonnet_142.txt', 'Sonnet_143.txt', 'Sonnet_144.txt', 'Sonnet_145.txt', 'Sonnet_146.txt', 'Sonnet_147.txt', 'Sonnet_148.txt', 'Sonnet_149.txt', 'Sonnet_150.txt', 'Sonnet_151.txt', 'Sonnet_152.txt', 'Sonnet_153.txt', 'Sonnet_154.txt']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
