{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo #2\n",
    "\n",
    "This demonstration is optional.  It goes over how to check and make sure that the data that we are using is not being corrupted or altered by the program that we're using.  It is important that as a data-scientist, you understand what the programs you are using actually do to your data, however, that is beside the purpose of this series of demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "try:\n",
    "    os.chdir(\"Sonnets\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training data\n",
    "I am calling the data \"Corpus\" this time to be consistent with the Gensim Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shakespeares-sonnets_TXT_FolgerShakespeare.txt\n",
      "stopwords.txt\n"
     ]
    }
   ],
   "source": [
    "corpus = os.listdir('.')\n",
    "for file in corpus:\n",
    "    if file.startswith(\"Sonnet\") and file.endswith(\".txt\"):\n",
    "        pass\n",
    "    else:\n",
    "        print(file)\n",
    "        corpus.remove(file)\n",
    "corpus.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 297)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_vectorizer = CountVectorizer(input='filename')\n",
    "corpus_sample = sample_vectorizer.fit_transform(corpus[0:5])\n",
    "numpy_sample = corpus_sample.toarray()\n",
    "numpy_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a close look at sonnet #1 and make sure the code's doing what we think it's doing by printing out the tokens for Sonnet #1, and then comparing it to the words for Sonnet #1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 3 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 2 2 0 0\n",
      " 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 0\n",
      " 1 0 0 0 1 0 0 1 0 0 2 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 1 0 1 2 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 2 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 2 0 0 0 0 1 0 0 0 0 0 0 0 1 2 0 0 0 1\n",
      " 0 0 0 0 0 1 0 0 2 0 2 6 1 0 0 0 1 0 2 1 0 2 0 0 0 4 1 0 1 4 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 3 0 0 0\n",
      " 0]\n",
      "(array([  0,   7,  12,  13,  18,  19,  21,  29,  31,  32,  33,  34,  39,\n",
      "        42,  43,  46,  47,  49,  53,  56,  64,  66,  69,  77,  80,  82,\n",
      "        83,  86,  88,  96,  97,  99, 100, 108, 111, 115, 118, 121, 127,\n",
      "       128, 140, 141, 152, 154, 156, 157, 165, 168, 173, 177, 178, 179,\n",
      "       180, 182, 197, 198, 203, 208, 216, 217, 221, 227, 230, 232, 233,\n",
      "       234, 238, 240, 241, 243, 247, 248, 250, 251, 254, 273, 274, 279,\n",
      "       288, 289, 292], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "print(numpy_sample[0])\n",
    "print(np.nonzero(numpy_sample[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From fairest creatures we desire increase,\n",
      "That thereby beauty's rose might never die,\n",
      "But, as the riper should by time decease,\n",
      "His tender heir might bear his memory.\n",
      "But thou, contracted to thine own bright eyes,\n",
      "Feed'st thy light's flame with self-substantial fuel,\n",
      "Making a famine where abundance lies,\n",
      "Thyself thy foe, to thy sweet self too cruel.\n",
      "Thou that art now the world's fresh ornament\n",
      "And only herald to the gaudy spring\n",
      "Within thine own bud buriest thy content\n",
      "And, tender churl, mak'st waste in niggarding.\n",
      "  Pity the world, or else this glutton be--\n",
      "  To eat the world's due, by the grave and thee.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sonnet1 = open('Sonnet_001.txt', 'r').read()\n",
    "print(Sonnet1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: The token numbers change each time the algorithm is run, you need to fill these in yourself from this array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "(array([  0,   7,  12,  13,  18,  19,  21,  29,  31,  32,  33,  34,  39,\n",
      "        42,  43,  46,  47,  49,  53,  56,  64,  66,  69,  77,  80,  82,\n",
      "        83,  86,  88,  96,  97,  99, 100, 108, 111, 115, 118, 121, 127,\n",
      "       128, 140, 141, 152, 154, 156, 157, 165, 168, 173, 177, 178, 179,\n",
      "       180, 182, 197, 198, 203, 208, 216, 217, 221, 227, 230, 232, 233,\n",
      "       234, 238, 240, 241, 243, 247, 248, 250, 251, 254, 273, 274, 279,\n",
      "       288, 289, 292], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "sonnet1_tokens = np.nonzero(numpy_sample[0])\n",
    "print(type(sonnet1_tokens))\n",
    "print(sonnet1_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = []\n",
    "for token in sonnet1_tokens[0]:\n",
    "    token_list.append(sample_vectorizer.get_feature_names()[token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check and make sure the sample we've extracted corresponds to the poem we extracted it from, so we know our analysis will be both accurate, and repeatable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n"
     ]
    }
   ],
   "source": [
    "for word in token_list:\n",
    "    if word in Sonnet1.lower():\n",
    "        print('Found')\n",
    "    else:\n",
    "        print('Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Missing\n",
      "Found\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Found\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Found\n",
      "Missing\n",
      "Found\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Found\n",
      "Found\n",
      "Missing\n",
      "Found\n",
      "Missing\n",
      "Found\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Found\n",
      "Missing\n",
      "Found\n",
      "Missing\n",
      "Found\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Found\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Missing\n",
      "Found\n",
      "Missing\n",
      "Missing\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Missing\n",
      "Missing\n",
      "Found\n",
      "Missing\n",
      "Missing\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Found\n",
      "Missing\n"
     ]
    }
   ],
   "source": [
    "Sonnet2 = open('Sonnet_002.txt', 'r').read()\n",
    "for word in token_list:\n",
    "    if word in Sonnet2.lower():\n",
    "        print('Found')\n",
    "    else:\n",
    "        print('Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
