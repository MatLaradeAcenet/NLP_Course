{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437553ea-aad7-4f45-acec-ad214518691d",
   "metadata": {},
   "source": [
    "# Demo #1 Bag Of Words\n",
    "\n",
    "Now that we have the sonnets into their own files, let's take them into an NLP library to try to get information about what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6616c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import os\n",
    "try:\n",
    "    os.chdir(\"Sonnets\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180da73b-9978-4dc6-886b-6c68e1c77d16",
   "metadata": {},
   "source": [
    "We need to make sure we're only grabbing the sonnets, but since we named them algorithmically, we can just pattern-match.  If you were working with less structured data, you may need to manually prune files out of the directory, or use some kind of regex pattern matching. Please note that if a random file (such as a model or parameters) makes its way into your data repo, unexpected things may happen, and you may get some strange errors about encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da25c913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shakespeares-sonnets_TXT_FolgerShakespeare.txt\n",
      "stopwords.txt\n"
     ]
    }
   ],
   "source": [
    "file_list = os.listdir('.')\n",
    "for file in file_list:\n",
    "    if file.startswith(\"Sonnet\") and file.endswith(\".txt\"):\n",
    "        pass\n",
    "    else:\n",
    "        print(file)\n",
    "        file_list.remove(file)\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932dbe50-11ba-4bd6-9b52-b9b3d1e59721",
   "metadata": {},
   "source": [
    "We must create an object to store the tokenizer and bag of words, then run the \"fit\" function to do the actual tokenization and counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4456fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(input='filename')\n",
    "vectorized_corpus = vectorizer.fit_transform(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78c40fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(vectorized_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ea61576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyzer = vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7adbd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154, 3074)\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_corpus.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed7b91-4bf2-452e-a121-7567c75672f3",
   "metadata": {},
   "source": [
    "### Viewing the data\n",
    "\n",
    "These are commands to dump out information about the data that we have loaded and processed.  They're commented out because they produce a lot of output, but feel free to uncomment them and run them yourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac936b2-eb28-40a4-987a-b13b9ab893f2",
   "metadata": {},
   "source": [
    "This command will give you all of the words in your text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a31c1a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4072c489-138e-4d4e-9e5e-d450672826e0",
   "metadata": {},
   "source": [
    "This command will print out the tokens in the corpus, in the form of\n",
    "\n",
    "(Sonnet, WordToken)     Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d86ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87d31da-7453-4589-ba92-d87f77548258",
   "metadata": {},
   "source": [
    "We can also access the tokens in the form of a matrix using numpy functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2564447b-8c5a-4a11-8b43-b12d312ca59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "<class 'numpy.ndarray'>\n",
      "(154, 3074)\n"
     ]
    }
   ],
   "source": [
    "array_corpus = vectorized_corpus.toarray() \n",
    "print(array_corpus)\n",
    "print(type(array_corpus))\n",
    "print(array_corpus.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaf3f7f-fc31-49d5-90d6-f3b68b0ca4f2",
   "metadata": {},
   "source": [
    "We can also access individual sonnets in the array by their index number, as they're the first axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aa63079-7868-47cf-bda3-bad84afc02e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "(array([   7,  101,  137,  139,  197,  199,  210,  350,  362,  371,  376,\n",
      "        378,  442,  516,  520,  556,  570,  617,  669,  692,  771,  800,\n",
      "        812,  909,  919,  930,  964, 1003, 1020, 1081, 1085, 1091, 1108,\n",
      "       1143, 1167, 1262, 1270, 1290, 1360, 1365, 1512, 1515, 1588, 1593,\n",
      "       1631, 1644, 1731, 1738, 1757, 1791, 1798, 1801, 1825, 1902, 2168,\n",
      "       2183, 2262, 2315, 2438, 2442, 2515, 2562, 2604, 2616, 2617, 2618,\n",
      "       2628, 2636, 2644, 2648, 2669, 2670, 2677, 2685, 2697, 2897, 2908,\n",
      "       2937, 2993, 2996, 3023], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "print(array_corpus[0])\n",
    "print(np.nonzero(array_corpus[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617ee00d-65a9-4c8f-8bb2-c1b74b1037cd",
   "metadata": {},
   "source": [
    "And we can access token information using the second index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f0e1cca-45b8-4459-bb43-2ebcb18fb9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abundance\n",
      "(array([  0,  22,  36, 134], dtype=int64),)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[7])\n",
    "print(np.nonzero(array_corpus[:,7]))\n",
    "print(np.sum(array_corpus[:,7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a629d-7a4b-4323-8070-bf59f19347cf",
   "metadata": {},
   "source": [
    "We can print out the sonnet to verify that the tokens we're looking at did indeed come from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57d60676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From fairest creatures we desire increase,\\n', \"That thereby beauty's rose might never die,\\n\", 'But, as the riper should by time decease,\\n', 'His tender heir might bear his memory.\\n', 'But thou, contracted to thine own bright eyes,\\n', \"Feed'st thy light's flame with self-substantial fuel,\\n\", 'Making a famine where abundance lies,\\n', 'Thyself thy foe, to thy sweet self too cruel.\\n', \"Thou that art now the world's fresh ornament\\n\", 'And only herald to the gaudy spring\\n', 'Within thine own bud buriest thy content\\n', \"And, tender churl, mak'st waste in niggarding.\\n\", '  Pity the world, or else this glutton be--\\n', \"  To eat the world's due, by the grave and thee.\\n\"]\n"
     ]
    }
   ],
   "source": [
    "print(open(file_list[0],'r').readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c2c24e-bf49-49e2-a43e-6ccdbff09455",
   "metadata": {},
   "source": [
    "### Implementing Stop Words\n",
    "We can add in our stop-words list here too, and when we re-run the vectorizer, note that while the number of sonnets stays the same, the vocabulary becomes smaller, as we would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dd7018a-31e4-47de-8d3f-5affa07e2e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'actually', 'almost', 'also', 'although', 'always', 'am', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'became', 'become', 'but', 'by', 'can', 'could', 'did', 'do', 'does', 'each', 'either', 'else', 'for', 'from', 'had', 'has', 'have', 'hence', 'how', 'i', 'if', 'in', 'is', 'it', 'its', 'just', 'may', 'maybe', 'me', 'might', 'mine', 'must', 'my', 'mine', 'must', 'my', 'neither', 'nor', 'not', 'of', 'oh', 'ok', 'when', 'where', 'whereas', 'wherever', 'whenever', 'whether', 'which', 'while', 'who', 'whom', 'whoever', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yes', 'yet', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "stop_words_file = open('stopwords.txt', 'r')\n",
    "stop_words = stop_words_file.read().splitlines() \n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93a7226c-b133-47cf-a8aa-fe5982813339",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_stop_words = CountVectorizer(input='filename', stop_words=stop_words)\n",
    "#Now we re-run the vectorizer above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e535a6c-e3ff-419a-93c9-83fa4f3a46d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154, 3012)\n"
     ]
    }
   ],
   "source": [
    "vectorized_stop_words =  vectorizer_stop_words.fit_transform(file_list)\n",
    "print(vectorized_stop_words.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f379045-3a28-407b-ac28-06713098afd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'neither', 'might', 'yet', 'my', 'has', 'ok', 'wherever', 'whereas', 'from', 'nor', 'am', 'when', 'oh', 'whom', 'almost', 'why', 'in', 'did', 'for', 'whenever', 'will', 'each', 'if', 'your', 'also', 'hence', 'become', 'with', 'had', 'must', 'me', 'just', 'you', 'about', 'who', 'any', 'as', 'mine', 'does', 'maybe', 'within', 'yes', 'could', 'which', 'would', 'at', 'a', 'its', 'without', 'can', 'although', 'an', 'do', 'always', 'are', 'but', 'actually', 'be', 'how', 'it', 'whether', 'have', 'may', 'of', 'whose', 'i', 'is', 'became', 'where', 'and', 'while', 'else', 'by', 'either', 'not', 'whoever'})\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer_stop_words.get_stop_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73727255-8ab8-4c04-a919-b50a5f1084fb",
   "metadata": {},
   "source": [
    "### n-Grams\n",
    "We can also create n-gram models using the same vectorizer module.\n",
    "\n",
    "In this example, we will use word tokens, and bi-grams (pairs of words) to just use bi-grams you would set the range to (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b244a1f-53d3-4336-a167-7e85c42b2f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_vectorizer = CountVectorizer(input='filename', ngram_range=(1,2), stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4dafaf6-3e8f-4a68-bdb5-81d348bb5741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154, 13818)\n"
     ]
    }
   ],
   "source": [
    "ngram_corpus = ngram_vectorizer.fit_transform(file_list)\n",
    "print(ngram_corpus.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82066010-3ea3-49c3-8f26-85420b71af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ngram_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4254d151-2cf3-45fd-9654-cbd7b6d2917c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
