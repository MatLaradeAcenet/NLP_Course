{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437553ea-aad7-4f45-acec-ad214518691d",
   "metadata": {},
   "source": [
    "# Demo #, Bag Of Words\n",
    "\n",
    "Now that we have the sonnets into their own files, let's take them into an NLP library to try to get information about what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6616c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "try:\n",
    "    os.chdir(\"Sonnets\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180da73b-9978-4dc6-886b-6c68e1c77d16",
   "metadata": {},
   "source": [
    "We need to make sure we're only grabbing the sonnets, but since we named them algorithmically, we can just pattern-match.  If you were working with less structured data, you may need to manually prune files out of the directory, or use some kind of regex pattern matching. Please note that if a random file (such as a model or parameters) makes its way into your data repo, unexpected things may happen, and you may get some strange errors about encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da25c913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shakespeares-sonnets_TXT_FolgerShakespeare.txt\n",
      "stopwords.txt\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(input='filename')\n",
    "file_list = os.listdir('.')\n",
    "for file in file_list:\n",
    "    if file.startswith(\"Sonnet\") and file.endswith(\".txt\"):\n",
    "        pass\n",
    "    else:\n",
    "        print(file)\n",
    "        file_list.remove(file)\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4456fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = vectorizer.fit_transform(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78c40fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(vectorized_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ea61576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyzer = vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7adbd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154, 3252)\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_corpus.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a31c1a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d86ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0807fc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154, 3252)\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_corpus.shape)\n",
    "#print(vectorized_corpus[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57d60676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from\n",
      "fairest\n",
      "creatures\n",
      "we\n",
      "['1\\n', '\\n', 'From fairest creatures we desire increase,\\n', \"That thereby beauty's rose might never die,\\n\", 'But, as the riper should by time decease,\\n', 'His tender heir might bear his memory.\\n', 'But thou, contracted to thine own bright eyes,\\n', \"Feed'st thy light's flame with self-substantial fuel,\\n\", 'Making a famine where abundance lies,\\n', 'Thyself thy foe, to thy sweet self too cruel.\\n', \"Thou that art now the world's fresh ornament\\n\", 'And only herald to the gaudy spring\\n', 'Within thine own bud buriest thy content\\n', \"And, tender churl, mak'st waste in niggarding.\\n\", '  Pity the world, or else this glutton be--\\n', \"  To eat the world's due, by the grave and thee.\\n\", '\\n']\n"
     ]
    }
   ],
   "source": [
    "#Double-Check these numbers, they seem to change eact time the tokenizer is run.\n",
    "print(vectorizer.get_feature_names()[1236])\n",
    "print(vectorizer.get_feature_names()[1069])\n",
    "print(vectorizer.get_feature_names()[705])\n",
    "print(vectorizer.get_feature_names()[3086])\n",
    "print(open(file_list[0],'r').readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40660d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teeming\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[2761])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0dd7018a-31e4-47de-8d3f-5affa07e2e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'actually', 'almost', 'also', 'although', 'always', 'am', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'became', 'become', 'but', 'by', 'can', 'could', 'did', 'do', 'does', 'each', 'either', 'else', 'for', 'from', 'had', 'has', 'have', 'hence', 'how', 'i', 'if', 'in', 'is', 'it', 'its', 'just', 'may', 'maybe', 'me', 'might', 'mine', 'must', 'my', 'mine', 'must', 'my', 'neither', 'nor', 'not', 'of', 'oh', 'ok', 'when', 'where', 'whereas', 'wherever', 'whenever', 'whether', 'which', 'while', 'who', 'whom', 'whoever', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yes', 'yet', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "stop_words_file = open('stopwords.txt', 'r')\n",
    "stop_words = stop_words_file.read().splitlines() \n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93a7226c-b133-47cf-a8aa-fe5982813339",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_stop_words = CountVectorizer(input='filename', stop_words=stop_words)\n",
    "#Now we re-run the vectorizer above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e535a6c-e3ff-419a-93c9-83fa4f3a46d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154, 3190)\n"
     ]
    }
   ],
   "source": [
    "vectorized_stop_words =  vectorizer_stop_words.fit_transform(file_list)\n",
    "print(vectorized_stop_words.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f379045-3a28-407b-ac28-06713098afd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'as', 'a', 'always', 'whose', 'would', 'yet', 'and', 'from', 'mine', 'also', 'not', 'who', 'how', 'why', 'whom', 'either', 'each', 'me', 'oh', 'wherever', 'whoever', 'do', 'in', 'does', 'actually', 'about', 'within', 'which', 'if', 'be', 'did', 'just', 'almost', 'maybe', 'my', 'neither', 'when', 'while', 'by', 'else', 'for', 'its', 'where', 'could', 'whereas', 'whenever', 'am', 'yes', 'i', 'became', 'although', 'might', 'at', 'with', 'have', 'may', 'it', 'are', 'must', 'of', 'has', 'ok', 'any', 'hence', 'nor', 'whether', 'is', 'your', 'without', 'become', 'but', 'an', 'can', 'will', 'you', 'had'})\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer_stop_words.get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b244a1f-53d3-4336-a167-7e85c42b2f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
